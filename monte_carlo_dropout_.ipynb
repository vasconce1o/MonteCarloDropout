{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "monte carlo dropout .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMU6ZNPAceocFOH1DIP3iR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasconce1o/MonteCarloDropout/blob/main/monte_carlo_dropout_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWu5NRzM-5sU"
      },
      "source": [
        "# XI TALLER INTERNACIONAL DE CIBERNÉTICA APLICADA\n",
        "# **Comprendiendo Monte Carlo Dropout para la representación de la incertidumbre en modelos de Aprendizaje Profundo**\n",
        "\n",
        "**Autor**: Yuniesky Orlando Vasconcelo Mir, yovasconcelo@xetid.cu\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6l9Hc8m7E8w"
      },
      "source": [
        "Después de entrenar durante aproximadamente 10 épocas, este modelo obtiene una precisión del 90,7% en el conjunto de pruebas. Para activar el dropout en el momento de la predicción, simplemente necesitamos establecer training=True comportamiento similar al entrenamiento, es decir, eliminar algunas neuronas. De esta manera, cada predicción será ligeramente diferente y podremos generar tantas como queramos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5QOEuZf3G0G",
        "outputId": "227bfd58-9141-40b6-bf04-9432253b8a30"
      },
      "source": [
        "from tensorflow import  keras\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=30)\n",
        "acc1 = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 4.0323 - accuracy: 0.7576\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5417 - accuracy: 0.8543\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4103 - accuracy: 0.8889\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3652 - accuracy: 0.9035\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3416 - accuracy: 0.9101\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3169 - accuracy: 0.9156\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3137 - accuracy: 0.9192\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2972 - accuracy: 0.9231\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2956 - accuracy: 0.9246\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2916 - accuracy: 0.9271\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2850 - accuracy: 0.9280\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2757 - accuracy: 0.9301\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2986 - accuracy: 0.9263\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2756 - accuracy: 0.9301\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2669 - accuracy: 0.9317\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2658 - accuracy: 0.9343\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2636 - accuracy: 0.9336\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2636 - accuracy: 0.9344\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2558 - accuracy: 0.9357\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2670 - accuracy: 0.9364\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2695 - accuracy: 0.9337\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2632 - accuracy: 0.9356\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2650 - accuracy: 0.9353\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2691 - accuracy: 0.9348\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2541 - accuracy: 0.9363\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2578 - accuracy: 0.9370\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2582 - accuracy: 0.9367\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2648 - accuracy: 0.9349\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2602 - accuracy: 0.9380\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2431 - accuracy: 0.9413\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1576 - accuracy: 0.9679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JrKg7xm7U7N"
      },
      "source": [
        "Creando dos funciones útiles: predict_proba() genera el número num_samples deseado de predicciones y promedia la probabilidad de clase predicha para cada uno de los 10 dígitos en el conjunto de datos MNIST, mientras que predict_class() simplemente elige la probabilidad predicha más alta para elegir la clase más probable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AMYl8po6XUb"
      },
      "source": [
        "def predict_proba(X, model, num_samples):\n",
        "    preds = [model(X, training=True) for _ in range(num_samples)]\n",
        "    return np.stack(preds).mean(axis=0)\n",
        "     \n",
        "def predict_class(X, model, num_samples):\n",
        "    proba_preds = predict_proba(X, model, num_samples)\n",
        "    return np.argmax(proba_preds, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msiXJElX8Czh"
      },
      "source": [
        "Ahora, hagamos 100 predicciones y evaluemos la precisión en el conjunto de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frxs9Bll6aAj",
        "outputId": "25997d7c-1739-4f77-d1c0-0329a24faaae"
      },
      "source": [
        "import numpy as np\n",
        "y_pred = predict_class(X_test, model, 100)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYwA8JA8kWZ"
      },
      "source": [
        "Esto arroja una precisión del 97,2%. En comparación con el resultado anterior, hemos disminuido la tasa de error del 3,3% al 2,8%, que es un 15%. Sin cambiar ni volver a entrenar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvNlG-Rm8xR4"
      },
      "source": [
        "# Monte Carlo Dropout\n",
        " Un vistazo a la incertidumbre de la predicción. \n",
        " En las tareas de clasificación, las probabilidades de clase obtenidas de la salida softmax a menudo se interpretan erróneamente como confianza del modelo. Sin embargo, Gal y Ghahramani (2016) muestran que un modelo puede ser incierto en sus predicciones incluso con una alta producción de softmax. También se puede observar en las predicciones de MNIST. Comparese la salida softmax con las probabilidades predichas por Monte Carlo Dropout para un solo ejemplo de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCxnPakJ9TCB",
        "outputId": "df96ad7e-ba58-407c-81d2-16743e45bb7a"
      },
      "source": [
        "y_pred_proba = predict_proba(X_test, model, 100)\n",
        "\n",
        "softmax_output = np.round(model.predict(X_test[1:2]), 3)\n",
        "mc_pred_proba = np.round(y_pred_proba[1], 3)\n",
        "print(softmax_output, mc_pred_proba)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] [0.029 0.002 0.956 0.003 0.002 0.001 0.    0.002 0.004 0.001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85IZqiCn99HJ"
      },
      "source": [
        "Ambos están de acuerdo en que el ejemplo de prueba es muy probable que sea de la 3ª clase. Sin embargo, el softmax está 100% seguro de que ese es el caso, lo que ya debería alertarte de que algo no está bien. Las estimaciones de probabilidad de 0% o 100% suelen ser peligrosas. Monte Carlo Dropout nos proporciona mucha más información sobre la incertidumbre de la predicción: lo más probable es que sea de clase 3, pero hay una pequeña posibilidad de que sea de clase 4, y 5, aunque poco probable, sigue siendo más probable que 1, por ejemplo."
      ]
    }
  ]
}